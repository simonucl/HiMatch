{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './bgc_train.json'\n",
    "dev_path = './bgc_val.json'\n",
    "test_path = './bgc_test.json'\n",
    "\n",
    "train_data, val_data, test_data = [], [], []\n",
    "with open(train_path) as fd:\n",
    "    for line in fd.readlines():\n",
    "        train_data.append(json.loads(line))\n",
    "with open(dev_path) as fd:\n",
    "    for line in fd.readlines():\n",
    "        val_data.append(json.loads(line))\n",
    "with open(test_path) as fd:\n",
    "    for line in fd.readlines():\n",
    "        test_data.append(json.loads(line))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': 'Title: The New York Times Daily Crossword Puzzles: Thursday, Volume 1. Text: Monday’s Crosswords Do with EaseTuesday’s Crosswords Not a BreezeWednesday’s Crosswords Harder StillThursday’s Crosswords Take Real SkillFriday’s Crosswords — You’ve Come This Far…Saturday’s Crosswords — You’re a Star!For millions of people, the New York Times crossword puzzles are as essential to each day as the first cup of coffee in the morning. Now, for the first time ever, these premier puzzles are available in six clever installments. With each day of the week, the puzzles increase gradually in skill level; Monday’s the easiest, but Saturday’s sure to challenge! Push your mental muscles a little harder each day with America’s favorite sophisticated — and fun — pastime: the New York Times crossword puzzles!The legendary Eugene T. Maleska was crossword editor of The New York Times from 1977 to 1993.',\n",
       " 'label': ['Nonfiction', 'Games']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max label length:  11\n",
      "min label length:  1\n",
      "mean label length:  3.009391255141794\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for data in train_data:\n",
    "    labels.append(len(data['label']))\n",
    "for data in val_data:\n",
    "    labels.append(len(data['label']))\n",
    "for data in test_data:\n",
    "    labels.append(len(data['label']))\n",
    "\n",
    "print('max label length: ', max(labels))\n",
    "print('min label length: ', min(labels))\n",
    "print('mean label length: ', np.mean(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58715/58715 [05:15<00:00, 186.21it/s]\n",
      "100%|██████████| 14785/14785 [01:20<00:00, 183.43it/s]\n",
      "100%|██████████| 18394/18394 [01:40<00:00, 182.31it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "def filter_non_tokens(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Define a regular expression pattern to match punctuation marks\n",
    "    punctuation_pattern = re.compile(r'^\\W*$')\n",
    "    \n",
    "    # Filter out punctuation marks\n",
    "    filtered_tokens = [token for token in tokens if not punctuation_pattern.match(token)]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def filter_stop_words(tokens):    \n",
    "    # Filter out stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in nltk.corpus.stopwords.words('english')]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "for i in trange(len(train_data)):\n",
    "    # train_data[i]['token'] = [str.lower(t).replace(',', '').replace('.', '').replace(\":\", \"\") for t in train_data[i]['token'].split()]\n",
    "    train_data[i]['token'] = filter_non_tokens(train_data[i]['token'])\n",
    "    train_data[i]['token'] = [str.lower(t).replace(',', '').replace('.', '').replace(\":\", \"\") for t in train_data[i]['token']]\n",
    "    train_data[i]['token'] = filter_stop_words(train_data[i]['token'])\n",
    "\n",
    "    # train_data[i]['label'] = [label_mapping[l] for l in train_data[i]['label']]\n",
    "\n",
    "for i in trange(len(val_data)):\n",
    "    # val_data[i]['token'] = [str.lower(t).replace(',', '').replace('.', '').replace(\":\", \"\") for t in val_data[i]['token'].split()]\n",
    "    val_data[i]['token'] = filter_non_tokens(val_data[i]['token'])\n",
    "    val_data[i]['token'] = [str.lower(t).replace(',', '').replace('.', '').replace(\":\", \"\") for t in val_data[i]['token']]\n",
    "\n",
    "    val_data[i]['token'] = filter_stop_words(val_data[i]['token'])\n",
    "    # val_data[i]['token'] = [str.lower(t).replace(',', '').replace('.', '') for t in val_data[i]['token']]\n",
    "\n",
    "for i in trange(len(test_data)):\n",
    "    # test_data[i]['token'] = [str.lower(t).replace(',', '').replace('.', '').replace(\":\", \"\") for t in test_data[i]['token'].split()]\n",
    "    test_data[i]['token'] = filter_non_tokens(test_data[i]['token'])\n",
    "    test_data[i]['token'] = [str.lower(t).replace(',', '').replace('.', '').replace(\":\", \"\") for t in test_data[i]['token']]\n",
    "    test_data[i]['token'] = filter_stop_words(test_data[i]['token'])\n",
    "    # test_data[i]['token'] = [str.lower(t).replace(',', '').replace('.', '') for t in test_data[i]['token']]\n",
    "    # test_data[i]['label'] = [label_mapping[l] for l in test_data[i]['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': ['title',\n",
       "  'new',\n",
       "  'york',\n",
       "  'times',\n",
       "  'daily',\n",
       "  'crossword',\n",
       "  'puzzles',\n",
       "  'thursday',\n",
       "  'volume',\n",
       "  '1',\n",
       "  'text',\n",
       "  'monday',\n",
       "  'crosswords',\n",
       "  'easetuesday',\n",
       "  'crosswords',\n",
       "  'breezewednesday',\n",
       "  'crosswords',\n",
       "  'harder',\n",
       "  'stillthursday',\n",
       "  'crosswords',\n",
       "  'take',\n",
       "  'real',\n",
       "  'skillfriday',\n",
       "  'crosswords',\n",
       "  'come',\n",
       "  'far…saturday',\n",
       "  'crosswords',\n",
       "  'star',\n",
       "  'millions',\n",
       "  'people',\n",
       "  'new',\n",
       "  'york',\n",
       "  'times',\n",
       "  'crossword',\n",
       "  'puzzles',\n",
       "  'essential',\n",
       "  'day',\n",
       "  'first',\n",
       "  'cup',\n",
       "  'coffee',\n",
       "  'morning',\n",
       "  'first',\n",
       "  'time',\n",
       "  'ever',\n",
       "  'premier',\n",
       "  'puzzles',\n",
       "  'available',\n",
       "  'six',\n",
       "  'clever',\n",
       "  'installments',\n",
       "  'day',\n",
       "  'week',\n",
       "  'puzzles',\n",
       "  'increase',\n",
       "  'gradually',\n",
       "  'skill',\n",
       "  'level',\n",
       "  'monday',\n",
       "  'easiest',\n",
       "  'saturday',\n",
       "  'sure',\n",
       "  'challenge',\n",
       "  'push',\n",
       "  'mental',\n",
       "  'muscles',\n",
       "  'little',\n",
       "  'harder',\n",
       "  'day',\n",
       "  'america',\n",
       "  'favorite',\n",
       "  'sophisticated',\n",
       "  'fun',\n",
       "  'pastime',\n",
       "  'new',\n",
       "  'york',\n",
       "  'times',\n",
       "  'crossword',\n",
       "  'puzzles',\n",
       "  'legendary',\n",
       "  'eugene',\n",
       "  'maleska',\n",
       "  'crossword',\n",
       "  'editor',\n",
       "  'new',\n",
       "  'york',\n",
       "  'times',\n",
       "  '1977',\n",
       "  '1993'],\n",
       " 'label': ['Nonfiction', 'Games']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump them into json files\n",
    "with open('./bgc_train.json', 'w') as fd:\n",
    "    for data in train_data:\n",
    "        json.dump(data, fd)\n",
    "        fd.write('\\n')\n",
    "\n",
    "with open('./bgc_val.json', 'w') as fd:\n",
    "    for data in val_data:\n",
    "        json.dump(data, fd)\n",
    "        fd.write('\\n')\n",
    "\n",
    "with open('./bgc_test.json', 'w') as fd:\n",
    "    for data in test_data:\n",
    "        json.dump(data, fd)\n",
    "        fd.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./bgc.taxonomy', 'r') as fd:\n",
    "    taxonomy = fd.readlines()\n",
    "\n",
    "for i in range(len(taxonomy)):\n",
    "    taxonomy[i] = '\\t'.join([label_mapping[l] if l != 'Root' else l for l in taxonomy[i].strip('\\n').split('\\t')])\n",
    "\n",
    "# dump the taxonomy into a file\n",
    "with open('./bgc.taxonomy', 'w') as fd:\n",
    "    for line in taxonomy:\n",
    "        fd.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "labels = []\n",
    "vocabs = []\n",
    "\n",
    "for data in train_data:\n",
    "    labels.extend(data['label'])\n",
    "    vocabs.extend(data['token'])\n",
    "\n",
    "# count the frequency of each label and vocab\n",
    "label_count = Counter(labels)\n",
    "vocab_count = Counter(vocabs)\n",
    "\n",
    "# sort the label and vocab by their frequency\n",
    "label_count = sorted(label_count.items(), key=lambda x: x[1], reverse=True)\n",
    "vocab_count = sorted(vocab_count.items(), key=lambda x: x[1], reverse=True)\n",
    "# save as vocab_bgc/label.dict and vocab_bgc/vocab.dict\n",
    "\n",
    "if not os.path.exists('./vocab_bgc'):\n",
    "    os.mkdir('./vocab_bgc')\n",
    "\n",
    "with open('./vocab_bgc/label.dict', 'w') as fd:\n",
    "    for label, count in label_count:\n",
    "        fd.write(label + '\\t' + str(count) + '\\n')\n",
    "\n",
    "with open('./vocab_bgc/vocab.dict', 'w') as fd:\n",
    "    for vocab, count in vocab_count:\n",
    "        fd.write(vocab + '\\t' + str(count) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
